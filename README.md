# Stimuli page for Interspeech 2023 submission:

## Overview
This companion page contains examples of the stimuli presented to participants in both the acoustic and lexical conditions, as well as an anonymous print of our previous work. 

## Anonymous preprint
We cite our previous work that will appear at EACL in 2023. As such, we provide an anonymnous transcript here (`EACL_print.pdf`)
> _Do dialogue representations align with perception? An empirical study_

## Stimuli 
We will provide the full set of stimuli from the Switchboard corpus along with the plausibility ratings after double blind review period. 

The survey was contructed through Qualtrics and presented to participants through Prolific Academic. 

Examples of the stimuli presentation are included here.
- Lexical condition: we provide a screenshot of both a stimuli question (`lexical_stimuli`), and a check question (`lexical_check`)
- Acoustic condition: [this link](https://edinburghinformatics.eu.qualtrics.com/jfe/form/SV_cBg4zwtjHYAlZB4) will take you to an example stimulus. PLEASE NOTE this link contains partially identifiable information. 

## Abstract
Speech is a fundamental means of communication and can be seen to provide two channels for transmitting information: the lexical channel of *which* words are said, and the non-lexical channel of *how* they are spoken. Both channels are known to shape listener expectations of upcoming communication; however, directly quantifying their relative contributions is difficult, with previous attempts requiring spoken variations of lexically-equivalent dialogue turns. This paper presents a generalised paradigm to study the value of non-lexical information across unconstrained lexical content. By quantifying the perceptual value of the non-lexical channel with both accuracy and entropy-reduction, we show that the non-lexical information produces a consistent effect on human expectations of upcoming dialogue, even when it leads to poorer discriminative turn judgements than in lexical-only conditions.
